{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e9589c",
   "metadata": {},
   "source": [
    "# Face Mask Detection — EDA and baseline classification\n",
    "Imports and environment setup for analysis and baseline model training.\n",
    "\n",
    "This notebook covers:\n",
    "- Dataset scan and stratified splits\n",
    "- Transforms and DataLoader\n",
    "- MobileNetV2 baseline training\n",
    "- Metrics, inference, and ONNX export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc30c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Core libs, plotting, ML stack, ONNX runtime, and utilities\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Reproducibility (best-effort): sets seeds for numpy/torch and non-deterministic ops\n",
    "# Note: full determinism may require disabling certain CUDA ops\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.use_deterministic_algorithms(False)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee502b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths and hyperparameters\n",
    "# If ROOT_DIR wasn't set earlier (e.g., by Colab setup), use local path\n",
    "from pathlib import Path\n",
    "try:\n",
    "    _ = ROOT_DIR\n",
    "except NameError:\n",
    "    ROOT_DIR = Path('/Users/usualcarl/Desktop/face_mask_detection')\n",
    "\n",
    "# Project directories\n",
    "DATA_DIR = ROOT_DIR / 'data'         # expects subfolders: with_mask/ and without_mask/\n",
    "MODELS_DIR = ROOT_DIR / 'models'     # trained models and ONNX exports\n",
    "REPORTS_DIR = ROOT_DIR / 'reports'   # plots, CSV indices, inference outputs\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "REPORTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Label mapping\n",
    "CLASS_TO_IDX = {'with_mask': 0, 'without_mask': 1}\n",
    "IDX_TO_CLASS = {v: k for k, v in CLASS_TO_IDX.items()}\n",
    "\n",
    "# Training config\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "LR = 1e-3\n",
    "NUM_WORKERS = 0  # Notebook-friendly; use >0 only when classes are importable from a module\n",
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142b8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Torch version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "# Device selection (CPU/GPU/MPS)\n",
    "# Prefer CUDA (NVIDIA) when available; on macOS, MPS (Apple Silicon) may accelerate training.\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Device:', device)\n",
    "print('Torch version:', torch.__version__)\n",
    "if device.type == 'cuda':\n",
    "    print('CUDA version:', torch.version.cuda)\n",
    "# Tip: Mixed precision is enabled only when device.type == 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60f83dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 7553\n",
      "class\n",
      "with_mask       3725\n",
      "without_mask    3828\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Dataset scan to index images and labels\n",
    "# Walks class directories and collects image paths and labels into a DataFrame.\n",
    "def scan_dataset(data_dir: Path):\n",
    "    rows = []\n",
    "    for cls in CLASS_TO_IDX.keys():\n",
    "        cls_dir = data_dir / cls\n",
    "        if not cls_dir.is_dir():\n",
    "            continue\n",
    "        for name in os.listdir(cls_dir):\n",
    "            if name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                rows.append({\n",
    "                    'path': str(cls_dir / name),  # absolute path to image\n",
    "                    'label': CLASS_TO_IDX[cls],   # numeric label\n",
    "                    'class': cls,                 # human-readable class\n",
    "                })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Build dataset index and save for reproducibility\n",
    "\n",
    "df = scan_dataset(DATA_DIR)\n",
    "print('Total images:', len(df))\n",
    "print(df.groupby('class').size())\n",
    "df.to_csv(REPORTS_DIR / 'dataset_index.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c520bcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes: 5287 1133 1133\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Stratified train/val/test split (70/15/15)\n",
    "# Keeps class proportions consistent across splits.\n",
    "train_df, temp_df = train_test_split(df, test_size=0.30, stratify=df['label'], random_state=SEED)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.50, stratify=temp_df['label'], random_state=SEED)\n",
    "print('Split sizes:', len(train_df), len(val_df), len(test_df))\n",
    "\n",
    "# Save split indices for later reuse (avoid data leakage across runs)\n",
    "train_df.to_csv(REPORTS_DIR / 'train_index.csv', index=False)\n",
    "val_df.to_csv(REPORTS_DIR / 'val_index.csv', index=False)\n",
    "test_df.to_csv(REPORTS_DIR / 'test_index.csv', index=False)\n",
    "\n",
    "# Class balance visualization\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='class', data=df)\n",
    "plt.title('Class distribution')\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'class_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e90746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentations and preprocessing\n",
    "# Transforms aim to improve generalization; normalization follows ImageNet statistics.\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),        # resize to model input\n",
    "    transforms.RandomHorizontalFlip(),              # simple augmentation\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),  # light color jitter\n",
    "    transforms.ToTensor(),                          # convert PIL→Tensor\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f809418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 224, 224])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataset and DataLoader\n",
    "# Simple image dataset using file list from DataFrame.\n",
    "class ImageListDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tfm=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tfm = tfm\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img = Image.open(row['path']).convert('RGB')\n",
    "        if self.tfm:\n",
    "            img = self.tfm(img)\n",
    "        label = int(row['label'])\n",
    "        return img, label\n",
    "\n",
    "# Construct datasets and loaders\n",
    "train_ds = ImageListDataset(train_df, train_tfms)\n",
    "val_ds = ImageListDataset(val_df, eval_tfms)\n",
    "test_ds = ImageListDataset(test_df, eval_tfms)\n",
    "\n",
    "# Note: pin_memory aids CUDA transfers; NUM_WORKERS=0 is notebook-friendly on macOS\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "# Sanity check: inspect shape of one batch\n",
    "next(iter(train_dl))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c30a127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 2226434\n"
     ]
    }
   ],
   "source": [
    "# Model (Transfer Learning: MobileNetV2)\n",
    "def create_model(num_classes=2, pretrained=True):\n",
    "    m = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.DEFAULT if pretrained else None)\n",
    "    in_features = m.classifier[1].in_features\n",
    "    m.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "model = create_model(num_classes=2, pretrained=True)\n",
    "model.to(device)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print('Trainable params:', trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, optimizer, scheduler, mixed precision (CUDA only)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1febc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     24\u001b[39m history = {\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mtrain_acc\u001b[39m\u001b[33m'\u001b[39m: [], \u001b[33m'\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m'\u001b[39m: []}\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     train_loss, train_acc = \u001b[43mrun_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     val_loss, val_acc = run_epoch(model, val_dl, train=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     28\u001b[39m     scheduler.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mrun_epoch\u001b[39m\u001b[34m(model, loader, train)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[32m     12\u001b[39m     optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m     scaler.step(optimizer)\n\u001b[32m     15\u001b[39m     scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/face_mask_detection/.venv/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    514\u001b[39m         Tensor.backward,\n\u001b[32m    515\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    520\u001b[39m         inputs=inputs,\n\u001b[32m    521\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/face_mask_detection/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    261\u001b[39m     retain_graph = create_graph\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    264\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training and validation loop\n",
    "def run_epoch(model, loader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    preds, targets = [], []\n",
    "    for xb, yb in tqdm(loader, leave=False):\n",
    "        xb, yb = xb.to(device), torch.tensor(yb).to(device)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
    "            out = model(xb)\n",
    "            loss = criterion(out, yb)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        preds.extend(out.argmax(1).detach().cpu().numpy().tolist())\n",
    "        targets.extend(yb.detach().cpu().numpy().tolist())\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    return avg_loss, acc\n",
    "\n",
    "best_f1 = 0.0\n",
    "history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = run_epoch(model, train_dl, train=True)\n",
    "    val_loss, val_acc = run_epoch(model, val_dl, train=False)\n",
    "    scheduler.step()\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | train_loss={train_loss:.4f} acc={train_acc:.3f} | val_loss={val_loss:.4f} acc={val_acc:.3f}\")\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1); plt.plot(history['train_loss'], label='train'); plt.plot(history['val_loss'], label='val'); plt.title('Loss'); plt.legend()\n",
    "plt.subplot(1,2,2); plt.plot(history['train_acc'], label='train'); plt.plot(history['val_acc'], label='val'); plt.title('Accuracy'); plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'training_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732e4d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics and confusion matrix\n",
    "model.eval()\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in tqdm(test_dl, leave=False):\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        all_preds.extend(out.argmax(1).detach().cpu().numpy().tolist())\n",
    "        all_targets.extend(torch.tensor(yb).detach().cpu().numpy().tolist())\n",
    "acc = accuracy_score(all_targets, all_preds)\n",
    "prec = precision_score(all_targets, all_preds, average='macro')\n",
    "rec = recall_score(all_targets, all_preds, average='macro')\n",
    "f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "print({'accuracy': acc, 'precision': prec, 'recall': rec, 'f1': f1})\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=list(CLASS_TO_IDX.keys()), yticklabels=list(CLASS_TO_IDX.keys()))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORTS_DIR / 'confusion_matrix.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07dcc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: single image and folder batch\n",
    "def preprocess_image(path):\n",
    "    img = Image.open(path).convert('RGB')\n",
    "    return eval_tfms(img).unsqueeze(0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_image(path):\n",
    "    xb = preprocess_image(path).to(device)\n",
    "    out = model(xb)\n",
    "    prob = torch.softmax(out, dim=1)[0].cpu().numpy()\n",
    "    pred = int(out.argmax(1)[0])\n",
    "    return {'path': path, 'pred': IDX_TO_CLASS[pred], 'probs': prob.tolist()}\n",
    "\n",
    "def predict_folder(dir_path):\n",
    "    results = []\n",
    "    for name in os.listdir(dir_path):\n",
    "        if name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            results.append(predict_image(str(Path(dir_path) / name)))\n",
    "    res_df = pd.DataFrame(results)\n",
    "    res_df.to_csv(REPORTS_DIR / 'inference_results.csv', index=False)\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade1ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONNX export and validation via onnxruntime\n",
    "def export_to_onnx(model, sample_shape=(1,3,IMG_SIZE,IMG_SIZE)):\n",
    "    model.eval()\n",
    "    dummy = torch.randn(sample_shape).to(device)\n",
    "    onnx_path = str(MODELS_DIR / 'mobilenet_v2_mask.onnx')\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy,\n",
    "        onnx_path,\n",
    "        input_names=['input'],\n",
    "        output_names=['logits'],\n",
    "        opset_version=12,\n",
    "        dynamic_axes={'input': {0: 'batch'}, 'logits': {0: 'batch'}},\n",
    "    )\n",
    "    print('Exported ONNX to', onnx_path)\n",
    "    return onnx_path\n",
    "\n",
    "def check_onnx(onnx_path):\n",
    "    sess = ort.InferenceSession(onnx_path, providers=['CPUExecutionProvider'])\n",
    "    dummy = np.random.randn(1,3,IMG_SIZE,IMG_SIZE).astype(np.float32)\n",
    "    out = sess.run(None, {'input': dummy})[0]\n",
    "    print('ONNX output shape:', out.shape)\n",
    "\n",
    "# Example:\n",
    "# onnx_file = export_to_onnx(model)\n",
    "# check_onnx(onnx_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit tests (quick checks)\n",
    "def test_transforms():\n",
    "    img = Image.new('RGB', (IMG_SIZE, IMG_SIZE), color=(255, 255, 255))\n",
    "    t = eval_tfms(img)\n",
    "    assert t.shape == (3, IMG_SIZE, IMG_SIZE)\n",
    "    print('Transforms test: OK')\n",
    "\n",
    "def test_dataset_item():\n",
    "    if len(train_df) > 0:\n",
    "        ds = ImageListDataset(train_df.iloc[:10], eval_tfms)\n",
    "        x, y = ds[0]\n",
    "        assert isinstance(y, int) and x.shape[0] == 3\n",
    "        print('Dataset item test: OK')\n",
    "\n",
    "def test_model_forward():\n",
    "    xb = torch.randn(2, 3, IMG_SIZE, IMG_SIZE)\n",
    "    out = model(xb.to(device))\n",
    "    assert out.shape == (2, 2)\n",
    "    print('Model forward test: OK')\n",
    "\n",
    "# Run quick tests (optional)\n",
    "test_transforms()\n",
    "test_dataset_item()\n",
    "test_model_forward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
